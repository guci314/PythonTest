{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "get_ipython().Completer.use_jedi = False\n",
    "from nltk.book import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.parse.corenlp import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLR caches the result of queries.', 'In this case the result set is very large even though you filter it and only return one row.']\n",
      "['SOLR', 'caches', 'the', 'result', 'of', 'queries', '.', 'In', 'this', 'case', 'the', 'result', 'set', 'is', 'very', 'large', 'even', 'though', 'you', 'filter', 'it', 'and', 'only', 'return', 'one', 'row', '.']\n",
      "['SOLR caches the result of queries.', 'In this case the result set is very large even though you filter it and only return one row.', 'guci']\n"
     ]
    }
   ],
   "source": [
    "s=\"SOLR caches the result of queries. In this case the result set is very large even though you filter it and only return one row.\"\n",
    "s1=\"2018年8月，因为做生意资金短缺，成都市民王倩向4款APP贷款了2万元。没想到，这笔贷款竟成为了她的噩梦，此后越滚越多，截至今年5月，王倩称她一共还款100多万元。\"\n",
    "sentence=sent_tokenize(s)\n",
    "print(sentence)\n",
    "words=word_tokenize(s)\n",
    "print(words)\n",
    "sent=sent_tokenize(s)\n",
    "sent.append(\"guci\")\n",
    "print(sent)\n",
    "#t=CoreNLPTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('addition', 'NN'), ('to', 'TO'), ('the', 'DT'), ('plaintext', 'NN'), ('corpora', 'NN'), (',', ','), ('NLTK', 'NNP'), (\"'s\", 'POS'), ('data', 'NN'), ('package', 'NN'), ('also', 'RB'), ('contains', 'VBZ'), ('a', 'DT'), ('wide', 'JJ'), ('variety', 'NN'), ('of', 'IN'), ('annotated', 'JJ'), ('corpora', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence=\"In addition to the plaintext corpora, NLTK's data package also contains a wide variety of annotated corpora.\"\n",
    "tokenized=nltk.word_tokenize(sentence)\n",
    "#print(tokenized)\n",
    "tagged=nltk.pos_tag(tokenized)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def f(self):\n",
    "        print('dfdfddf')\n",
    "    def f1(self):\n",
    "        print('dfdfddf')\n",
    "\n",
    "def my_matches(test):\n",
    "    # might want to be smarter here\n",
    "    return ['angela@domain.com', 'michael@domain.com', 'david@test.com']\n",
    "ip = get_ipython()\n",
    "ip.Completer.matchers.append(my_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfdfddf\n"
     ]
    }
   ],
   "source": [
    "a=A() # type:A\n",
    "a.f1()\n",
    "a.x=\"fdfdf\"\n",
    "a.y=\"dfdfd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('/home/guci/stanford-corenlp-full-2018-10-05',lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (IP\n",
      "    (NP\n",
      "      (DNP\n",
      "        (NP (NR 中国))\n",
      "        (DEG 的))\n",
      "      (NP (NN 首都)))\n",
      "    (VP (VV 在)\n",
      "      (NP (PN 哪里)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = '達沃斯世界經濟論壇是每年全球政商界領袖聚在一起的年度盛事。'\n",
    "#x=nlp.word_tokenize(sentence)\n",
    "#x=nlp.pos_tag(sentence)\n",
    "#x=nlp.ner(sentence)\n",
    "#print(x)\n",
    "sentence1='中国的首都在哪里'\n",
    "sentence2='你是猪'\n",
    "from nltk.tree import Tree\n",
    "s=nlp.parse(sentence1)\n",
    "print(s)\n",
    "tree=Tree.fromstring(s)\n",
    "tree.draw()\n",
    "\n",
    "\n",
    "# def printTree(t):\n",
    "#     for child in tree:\n",
    "#         print(child.label())\n",
    "#         if isinstance(child, Tree):\n",
    "#             printTree(child.node)\n",
    "\n",
    "            \n",
    "# printTree(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/guci/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/guci/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt', 'pretrain_path': '/home/guci/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/guci/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt', 'pretrain_path': '/home/guci/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "pipeline = stanfordnlp.Pipeline(lang='zh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token index=1;words=[<Word index=1;text=達沃斯;lemma=達沃斯;upos=PROPN;xpos=NNP;feats=_;governor=4;dependency_relation=nmod>]>\n",
      "<Token index=2;words=[<Word index=2;text=世界;lemma=世界;upos=NOUN;xpos=NN;feats=_;governor=4;dependency_relation=nmod>]>\n",
      "<Token index=3;words=[<Word index=3;text=經濟;lemma=經濟;upos=NOUN;xpos=NN;feats=_;governor=4;dependency_relation=nmod>]>\n",
      "<Token index=4;words=[<Word index=4;text=論壇;lemma=論壇;upos=NOUN;xpos=NN;feats=_;governor=16;dependency_relation=nsubj>]>\n",
      "<Token index=5;words=[<Word index=5;text=是;lemma=是;upos=AUX;xpos=VC;feats=_;governor=16;dependency_relation=cop>]>\n",
      "<Token index=6;words=[<Word index=6;text=每年;lemma=每年;upos=DET;xpos=DT;feats=_;governor=10;dependency_relation=nmod>]>\n",
      "<Token index=7;words=[<Word index=7;text=全球;lemma=全球;upos=NOUN;xpos=NN;feats=_;governor=10;dependency_relation=nmod>]>\n",
      "<Token index=8;words=[<Word index=8;text=政;lemma=政;upos=NOUN;xpos=NN;feats=_;governor=10;dependency_relation=nmod>]>\n",
      "<Token index=9;words=[<Word index=9;text=商界;lemma=商界;upos=NOUN;xpos=NN;feats=_;governor=10;dependency_relation=nmod>]>\n",
      "<Token index=10;words=[<Word index=10;text=領袖;lemma=領袖;upos=NOUN;xpos=NN;feats=_;governor=11;dependency_relation=nsubj>]>\n",
      "<Token index=11;words=[<Word index=11;text=聚;lemma=聚;upos=VERB;xpos=VV;feats=_;governor=16;dependency_relation=acl:relcl>]>\n",
      "<Token index=12;words=[<Word index=12;text=在;lemma=在;upos=VERB;xpos=VV;feats=_;governor=11;dependency_relation=mark>]>\n",
      "<Token index=13;words=[<Word index=13;text=一起;lemma=一起;upos=NOUN;xpos=NN;feats=_;governor=11;dependency_relation=obj>]>\n",
      "<Token index=14;words=[<Word index=14;text=的;lemma=的;upos=PART;xpos=DEC;feats=_;governor=11;dependency_relation=mark:relcl>]>\n",
      "<Token index=15;words=[<Word index=15;text=年度;lemma=年度;upos=NOUN;xpos=NN;feats=_;governor=16;dependency_relation=nmod>]>\n",
      "<Token index=16;words=[<Word index=16;text=盛事;lemma=盛事;upos=NOUN;xpos=NN;feats=_;governor=0;dependency_relation=root>]>\n",
      "<Token index=17;words=[<Word index=17;text=。;lemma=。;upos=PUNCT;xpos=.;feats=_;governor=16;dependency_relation=punct>]>\n",
      "('達沃斯', '4', 'nmod')\n",
      "('世界', '4', 'nmod')\n",
      "('經濟', '4', 'nmod')\n",
      "('論壇', '16', 'nsubj')\n",
      "('是', '16', 'cop')\n",
      "('每年', '10', 'nmod')\n",
      "('全球', '10', 'nmod')\n",
      "('政', '10', 'nmod')\n",
      "('商界', '10', 'nmod')\n",
      "('領袖', '11', 'nsubj')\n",
      "('聚', '16', 'acl:relcl')\n",
      "('在', '11', 'mark')\n",
      "('一起', '11', 'obj')\n",
      "('的', '11', 'mark:relcl')\n",
      "('年度', '16', 'nmod')\n",
      "('盛事', '0', 'root')\n",
      "('。', '16', 'punct')\n"
     ]
    }
   ],
   "source": [
    "s='達沃斯世界經濟論壇是每年全球政商界領袖聚在一起的年度盛事。'\n",
    "s1='算法已经在图像和音频领域取得了惊人的成果.我觉得其中最有趣也是最基本的，就是词向量'\n",
    "doc = pipeline(s)\n",
    "doc.sentences[0].print_tokens()\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Word index=1;text=\";lemma=\";upos=PUNCT;xpos=``;feats=_;governor=2;dependency_relation=punct>,\n",
       " <Word index=2;text=I;lemma=I;upos=X;xpos=FW;feats=_;governor=0;dependency_relation=root>,\n",
       " <Word index=3;text=love;lemma=love;upos=X;xpos=FW;feats=_;governor=2;dependency_relation=flat:foreign>,\n",
       " <Word index=4;text=you;lemma=you;upos=X;xpos=FW;feats=_;governor=2;dependency_relation=flat:foreign>,\n",
       " <Word index=5;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=2;dependency_relation=punct>,\n",
       " <Word index=6;text=I;lemma=I;upos=X;xpos=FW;feats=_;governor=2;dependency_relation=flat:foreign>,\n",
       " <Word index=7;text=hate;lemma=hate;upos=X;xpos=FW;feats=_;governor=6;dependency_relation=flat:foreign>,\n",
       " <Word index=8;text=him;lemma=him;upos=X;xpos=FW;feats=_;governor=6;dependency_relation=flat:foreign>,\n",
       " <Word index=9;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=2;dependency_relation=punct>,\n",
       " <Word index=10;text=You;lemma=You;upos=X;xpos=FW;feats=_;governor=6;dependency_relation=flat:foreign>,\n",
       " <Word index=11;text=are;lemma=are;upos=X;xpos=FW;feats=_;governor=10;dependency_relation=flat:foreign>,\n",
       " <Word index=12;text=nice;lemma=nice;upos=X;xpos=FW;feats=_;governor=10;dependency_relation=flat:foreign>,\n",
       " <Word index=13;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=14;dependency_relation=punct>,\n",
       " <Word index=14;text=He;lemma=He;upos=X;xpos=FW;feats=_;governor=2;dependency_relation=flat:foreign>,\n",
       " <Word index=15;text=is;lemma=is;upos=X;xpos=FW;feats=_;governor=14;dependency_relation=flat:foreign>,\n",
       " <Word index=16;text=dumb\";lemma=dumb\";upos=X;xpos=FW;feats=_;governor=14;dependency_relation=punct>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1=pipeline('\"I love you. I hate him. You are nice. He is dumb\"')\n",
    "doc1.sentences[0].words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  With/IN\n",
      "  (PERSON Python/NNP)\n",
      "  and/CC\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "s=\"Natural Language Processing With Python and NLTK .\"\n",
    "words=nltk.word_tokenize(s)\n",
    "tagged=nltk.pos_tag(words)\n",
    "namedEntity=nltk.ne_chunk(tagged)\n",
    "namedEntity.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "get_ipython().Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open    High     Low    Close      Volume  Ex-Dividend  \\\n",
      "Date                                                                   \n",
      "2004-08-19  100.01  104.06   95.96  100.335  44659000.0          0.0   \n",
      "2004-08-20  101.01  109.08  100.50  108.310  22834300.0          0.0   \n",
      "2004-08-23  110.76  113.48  109.05  109.400  18256100.0          0.0   \n",
      "2004-08-24  111.24  111.60  103.57  104.870  15247300.0          0.0   \n",
      "2004-08-25  104.76  108.00  103.88  106.000   9188600.0          0.0   \n",
      "\n",
      "            Split Ratio  Adj. Open  Adj. High   Adj. Low  Adj. Close  \\\n",
      "Date                                                                   \n",
      "2004-08-19          1.0  50.159839  52.191109  48.128568   50.322842   \n",
      "2004-08-20          1.0  50.661387  54.708881  50.405597   54.322689   \n",
      "2004-08-23          1.0  55.551482  56.915693  54.693835   54.869377   \n",
      "2004-08-24          1.0  55.792225  55.972783  51.945350   52.597363   \n",
      "2004-08-25          1.0  52.542193  54.167209  52.100830   53.164113   \n",
      "\n",
      "            Adj. Volume  \n",
      "Date                     \n",
      "2004-08-19   44659000.0  \n",
      "2004-08-20   22834300.0  \n",
      "2004-08-23   18256100.0  \n",
      "2004-08-24   15247300.0  \n",
      "2004-08-25    9188600.0  \n",
      "            Adj. Close    HL_PCT  PCT_change  Adj. Volume\n",
      "Date                                                     \n",
      "2004-08-19   50.322842  8.072956    0.324968   44659000.0\n",
      "2004-08-20   54.322689  7.921706    7.227007   22834300.0\n",
      "2004-08-23   54.869377  4.049360   -1.227880   18256100.0\n",
      "2004-08-24   52.597363  7.657099   -5.726357   15247300.0\n",
      "2004-08-25   53.164113  3.886792    1.183658    9188600.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import quandl\n",
    "\n",
    "df = quandl.get(\"WIKI/GOOGL\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df = df[['Adj. Open',  'Adj. High',  'Adj. Low',  'Adj. Close', 'Adj. Volume']]\n",
    "df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Close'] * 100.0\n",
    "df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0\n",
    "df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']]\n",
    "print(df.head())\n",
    "s='dfdfdf'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
